# -*- coding: utf-8 -*-
"""Step3_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ymARd1w3SMjVC_GDNL0hYI26l7NBuELP
"""
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import LogisticRegressionCV
from sklearn import svm

FINAL_DATASET_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), '/')
wells = pd.read_csv('/home/adel-saadeddin/PycharmProjects/demoapp/space/data_w_faults.csv')

X = wells[['fault1', 'fault2', 'fault3', 'fault4', 'fault5', 'fault6']]
y = wells['target']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

vanilla = LogisticRegression()
vanilla.fit(X_train, y_train)
acc_v = vanilla.score(X_test, y_test)
print(acc_v)

"""Expected worse for first try vanilla Logit. Anyway a lot of things are probably fucked up in the data, lots of the fault data are pretty similar because wells are close together.


I'm quite sure that going with NN with only 6 features and 198 observations is a bad idea. I'm even hestinat to add regularization. I'm even hestinant of adding any regularization to logit.
"""

choco = LogisticRegressionCV()
choco.fit(X_train, y_train)
acc_c = choco.score(X_test, y_test)
print(acc_c)


mclf = svm.SVC()
mclf.fit(X_train, y_train)
acc_m = mclf.score(X_test, y_test)
print(acc_m)

"""Not bad. I think performance is not that important for this course. We will need to focus more on the presentation 
"""


# TODO: ideally, we should set up a getter method where we give it a set of coordinates,
# computes faults and does a forward pass on the model

def predict(lat, lng):
    return mclf.predict([lat, lng])
